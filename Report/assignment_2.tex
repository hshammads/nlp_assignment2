\documentclass[twocolumn,12pt]{article}
\usepackage{ragged2e}
\usepackage{csquotes}
\usepackage{amsmath}
\usepackage{array}
\newcolumntype{C}{>$c<$}
\usepackage{mathtools}
%\usepackage{float}
\usepackage{hyperref}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{bigints}
\usepackage{esvect}
\usepackage{pgfplots}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{multicol}
\usepackage{nonfloat}
\usepackage{wrapfig}
\newcommand*\circled[1]{\tikz[baseline=(char.base)]{
            \node[shape=circle,draw,inner sep=2pt] (char) {#1};}}
\usepackage{textcomp}
\RequirePackage{color,graphicx}
\usepackage{graphicx,subcaption}
\usepackage{caption}
\usepackage[paper=letterpaper,margin=0.8in]{geometry}
\newcommand\myeq{\stackrel{\mathclap{\normalfont\mbox{L'H}}}{=}}
\definecolor{amaranth}{rgb}{0.9, 0.17, 0.31}
\usepackage[noabbrev,capitalise]{cleveref}
\usepackage{titlesec}

\DeclareRobustCommand{\abbrevcrefs}{%
\crefname{figure}{fig.}{figs.}%
\crefname{equation}{eqn.}{eqns.}%
%\crefname{table}{tbl.}%
}

\DeclareRobustCommand{\cshref}[1]{{\abbrevcrefs\cref{#1}}}

%custom figure code for multicol
\makeatletter
\newenvironment{tablehere}
  {\def\@captype{table}}
  {}

\newenvironment{figurehere}
  {\def\@captype{figure}}
  {}
  
\newenvironment{subfigurehere}
  {\def\@captype{subfloat}}
  {}
\makeatother

\begin{document}

\onecolumn
%---------Put name and information on the top right-----------%
\centering{\huge Assignment 2}\\~\\

%\null\hfill\begin{tabular}[t]{l@{}}
%\textbf{Hammad Ahmed Sheikh}\\
%\textsc{hshammads@csu.fullerton.edu}\\
\textsc{cpsc 488: natural language processing}\\
\textsc{10/14/2024}\\
\textsc{dr. christopher ryu}\\
\textsc{california state university, fullerton (CSUF)}\\~\\
%\textsc{November 5, 2023}\\~\\~\\
%\end{tabular}

%---------body of the pitch-----------%
%\begin{multicols}{2}
\begin{flushleft}
\justifying

\section*{Assignment Information}
\textcolor{red}{OPTIONAL:} This assignment can be completed individually or by a team with up to 5 members.\\
Total score: 60\\
Due date: 12/09/2024 11:59PM\\
Members: Hammad Sheikh, Matthew Do, Ryan Avancena \\
Work distribution: Equal

\iffalse
\subsection*{Objectives}
In this exercise, you will learn how to develop a sentiment analysis model and an automated trading system relying on the analysis results.
\textcolor{blue}{Only Python programs written using Python 3.0 or higher will be accepted}. \textcolor{red}{NO Jupyter notebook or any other Python variants} will be accepted for efficient grading.\\
\textbf{Change: get news articles for at least 4 weeks for all the members of ETF FNGU}

$\hrulefill$
\subsection*{About the dataset and problem to solve}
A stock, in investment terms, represents ownership in a company. When you buy stocks, you essentially buy a small piece of that company. The stock price for a company is mainly determined by the current value of the company that can be computed by the company’s past financial data (e.g., earnings, revenues, profits, etc.) and future value determined by the company’s future performance, which is unknown. The intrinsic value of a company refers to the actual and inherent worth of the company based on both current and future value. Therefore, a stock price is determined by the current value and estimated (perceived) future value of the company. Investors buy stocks based on the company’s estimated intrinsic value. That’s why the stock prices inherently fluctuate depending on the market situation and the company’s future prospectus, which are typically published in news articles. The more information investors have, the better they can estimate the intrinsic value.\\
An Exchange-Traded Fund (\textbf{ETF}) is a type of investment fund that holds a collection of assets such as stocks, bonds, commodities, or a mix of these. There are many ETFs covering specific sections of the market currently being traded in the market. Some ETFs are conservative (slow price change) and some are highly aggressive or volatile in price change. \textbf{FNGU} is one of the ETF funds that concentrates on the top 10 popular technology companies, also known as \textbf{FANG+}, which include META, AAPL, AMZN, NFLX, NVDA, GOOGL, MSFT, CRWD, AVGO, and \href{https://finance.yahoo.com/quote/FNGU/holdings/}{NOW}. The fund manager occasionally changes the holdings. FNGU is highly volatile due to high leverage (3x). The price of FNGU change when the price of any of these holdings change. As a pair of FNGU, \textbf{FNGD} follows the inverse of FNGU for those who want to bet against FNGU. If most of the holdings are up, FNGU will go up 3x, but FNGD will go down 3x, and vice versa. In other words, the price movement of FNGU and FNGD is inverse, so you can buy and sell either one for profit in either direction as long as you can correctly predict the direction without relying on a sell-short strategy. Due to the popularity, the size, and their technological innovations of all those companies in FNGU, they most likely generate lots of news articles that impact the price of FNGU as well as the market indices.\\
In this assignment, you will analyze the FNGU price data and related news articles to develop a simple trading system. Your trading system will be equipped with a sentiment analysis model that analyzes all the (directly or indirectly) FNGU-related news articles for \textcolor{red}{at least four weeks}, computes its market impact, and trades only FNGU, FNGD, or both FNGU and FNGD based on the impact. For example, your trading system will buy a certain number of shares if the news article is considered positive and the balance is enough to cover the trade. Otherwise, the system will sell a certain number of shares if the news article is negative and a sufficient number of shares are already owned. The \textcolor{green}{ultimate goal} is maximizing the \textcolor{green}{return} from the initial investment, utilizing the sentiment analysis model.

\subsection*{Required activities}
\textcolor{red}{\textbf{Utilize} the \textbf{GPU} resources available on the \textbf{Nautilus} through the \textbf{Kubernetes}} for modeling (\textbf{NOT} by the JupyterHub) and \textcolor{red}{\textbf{write} an analysis report} about your system's modeling results and trading performance by answering all the questions below with your \textbf{\textcolor{red}{justification} supported \textcolor{red}{by the data}}.

\begin{enumerate}
	\item \textbf{Download} the historical price data for FNGU (or FNGD or both) and its (directly or indirectly) related news articles for your selected period of four weeks from any free data sources (e.g., Yahoo Finance) and save the data in a JSON file format on your local machine. JSON is an open standard file format used for data storage or exchange. The price data should include market date, open price, high price, low price, closing price, and volume. The news articles may be related to the entire market, not necessarily only to FANG+. The direct FANG+-related news can be articles about the companies that you can download from the data source. Examples of indirect FANG+-related news can be the news about the market (Dow Jones \scalebox{.8}{\textsuperscript{$\wedge$}}DJI, S\&P \scalebox{.8}{\textsuperscript{$\wedge$}}GSPC, Nasdaq \scalebox{.8}{\textsuperscript{$\wedge$}}IXIC, or the industry FANG+ belongs to). Briefly describe the types of news articles you downloaded for your system, explaining why and how you downloaded the data.\\
	
	\item \textbf{Develop} a sentiment analysis model using Multilayer Perceptron (MLP) that can quantitatively estimate the impact of each news article on the FNGU price. Briefly describe 
		\subitem (a) The methods used to (pre)process the news and price data for your MLP algorithm with justification and 
		\subitem (b) the method(s) used to analyze the news and quantify its impact that will be used for trade.
	
	\item \textbf{Backtest} your trading system based on the sentiment analysis model developed in (2) and measure its performance. Backtesting means testing the effectiveness of trading systems that utilize specific trading algorithms. In this case, your sentiment analysis model is a trading algorithm. For the backtesting, assume your account has an initial investment balance of \$100,000, and buy/sell orders will always be filled without trading fees. 

	To evaluate the performance of your trading algorithm, you need to \textbf{develop a simple trading system} that will buy a certain number of FNGU shares only if the balance is sufficient to cover the purchase and the positive market impact computed by the model or sell a certain number of shares only if it already holds enough number of shares (\textcolor{red}{no short sell allowed}) and the negative market impact. 
	\textbf{Evaluate your system’s trading performance} \underline{so far} by calculating the following simple metrics\\
		\subitem (a) \$gain or \$loss for each trade, 
		\subitem (b) the total $gain or $loss for all trades, 
		\subitem (c) \% return compared to the initial balance. 
		
	\textbf{Log every trade}, including the key transaction data such as\\
		\subitem (a) the transaction date, 
		\subitem (b) trading type buy/sell, 
		\subitem (c) \# of shares traded, 
		\subitem (d) \$amount used for the trade, and 
		\subitem (e) the current balance after the trade to a log file \enquote{\textcolor{purple}{trade\_log.json}} for future analysis, verification, or accounting purpose.
		
	\textbf{Display the trading summary}, including the 
		\subitem (a) total \$gain or \$loss for all trades and
		\subitem (b) \% return compared to the initial balance (\$100,000.00).

	\item \textbf{Briefly describe} at least two methods or techniques (based on the relevant topics discussed in class) to improve the model performance and evaluation results on whether or not those methods improved the trading performance.
	
	\item \textbf{Create word embeddings} based on Word2vec, other embedding method, or pre-trained embedding for your model and compare the trading performance with the best model without relying on word embeddings.
\end{enumerate}

\textcolor{red}{\textbf{Warning:}} Although you can reuse any source codes available on the Internet, you are not allowed to share your codes with any other team or students in this class. Any student or team violating this policy will receive a \textbf{ZERO} score for this assignment, potentially for all the remaining assignments.

\subsection*{What to submit}
\begin{itemize}
	\item \textcolor{blue}{One analysis report} includes \textcolor{blue}{all your member names}, \% contribution made by each member, and all the answers to the questions based on your analysis in \textbf{PDF} or \textbf{Word format}. If every member contributed equally, simply state \enquote{\textcolor{purple}{equal contribution}.} If your team does not agree on individual contributions, briefly write a task description for each member. Different grades may be assigned based on individual contributions, even if a group completed the work.
	\item \textbf{Upload one analysis report file} and \textbf{Python program file(s)}, individually. Please \textcolor{red}{DO NOT upload any zip file} since Canvas cannot open it.
	\item When you show some example data in your analysis report (when necessary), select only a few examples, not including the entire dataset.
	\item \textbf{Submit only one for each team.}
\end{itemize}

\subsection*{Grading criteria}
\begin{itemize}
	\item The overall quality of work shown in the report about modeling results, supporting data, analysis process, methods used, and correctly implemented programs
	\item The level of understanding as reflected in the report
	\item Effort (10\%)
\end{itemize}
\fi

$\hrulefill$

\textit{Abstract} - Machine learning (ML) and Artificial Intelligence (AI) worlds are experiences tremendous growth. Their applications are uncountable. One realm of applications for ML and AI is Natural Language Processing (NLP). In this aspect, there are various classification
models that assist with text classification and processing. Our goal for this assignment is to utilize NLP techniques in analyzing news data for ETF FNGU to build a ML model that would maximize our investment returns. The stock data can be obtained via the Python libraries that Dr. Ryu \cite{cryu} defined. News articles will be scraped from the web.\\
\textit{Keywords} - Stock Trading, ETF, FNGU, Natural Language Processing, Machine Learning

	\tableofcontents
	
	\section{Introduction} \label{introduction}
	There is a lot of stock data available in the world. All the stocks are impacted by the environment and the economical status of the world. This information is shared via news articles, which are in text format. We need to able to analyze news data, identify patterns and sentiment, and utilize it to predict impact on stocks. However, due to the sheer size of data available and the amount of convolution, it is not an easy task for a human. If we can
utilize the computing power and technology available in computers, we may be able to make an impact. This may not have been possible a few decades ago. However, with the expansion of ML and AI technologies, numerous applications become available. With the use of NLP, we can perform text classification and sentiment analysis.\\
	Without generalizing too much, let us look at an ETF, ticker FNGU, with the goal of applying existing text classification and sentiment analysis algorithms on news and stock data, and analyzing their results for performance and accuracy to maximize our investment returns. 
	
	\section{Approach} \label{approach}
	The generalized approach is shown below in \cref{fig:gen_approach}. (Data Acquisition $\rightarrow$ Data Pre-Processing $\rightarrow$ Sentiment Analysis and Model Development $\leftrightarrow$ Model Evulation.) Further details will be discussed in the following sections. 

	\begin{figurehere}
		\centering
		\includegraphics[width=0.6\linewidth]{images/gen_approachv2.png}
		\caption{Generalized Approach}
		\label{fig:gen_approach}
	\end{figurehere}
	
	We will be relying on existing libraries as much as possible. This will help to focus on the classification and sentiment analysis task instead of building code from scratch for things that already exist. For example, train\_test\_split from sklearn.model\_selection \cite{scdatasplit} will be utilized to split the data into training and testing datasets. We also tried different methodologies in order to test and assist each other. There are two main methodologies; one primarily developed by Matthew Do, that will now be referenced as \enquote{Method A} \cite{metha}, and one primarily developed by Ryan Avancena, that will now be referenced as \enquote{Method B} \cite{methb}.
		
	\subsection{Data Acquisition} \label{data_acquisition}
	%\textit{\underline{Note from Dr. Ryu:}} You can use any package to download the data, but some popular Python packages may be \textbf{yfinance} or \textbf{yahoo-fin} or \textbf{both}. You \textbf{don't want} to use \textbf{rudimentary approaches like web scrapping as it will take too long time, and you are not sure about the data quality.}\\
	The first step in data processing, regardless of whether it is for our assignment or any other project, is data acquisition. We need to have a dataset that can be used to train need based models.\\
	Our assignment needs two main datasets; one for stock information and one for news articles. For the ETF information, we utilized Python library yfinance \cite{yfin} to download the data and stored it locally in JSON files. For the news articles, we utilized \textcolor{blue}{get\_news()} function of the yfinance library \cite{yfin} to grab the URLs and then grabbed the articles text and information via HTML parser, modified from Dr. Ryu's \cite{cryu} code. All the information grabbed is stored in appropriate JSON files to be utilized in model development. This enables us to have a defined set of data, which we can preprocess and utilize in our model development of sentiment analysis and investment returns.\\
	\cref{fig:fngu_hist} shows a sample snapshot of FNGU market data acquired from yfinance \cite{yfin}, and \cref{fig:news_data} shows a sample snapshot of news data scraped from the web via our HTML scraper.
	
	\mbox{}
	\begin{figurehere}
		\centering
		\includegraphics[width=\linewidth]{images/fngu_hist.png}
		\caption{Sample snapshot of FNGU market data}
		\label{fig:fngu_hist}
	\end{figurehere}
	
	\mbox{}
	\begin{figurehere}
		\centering
		\includegraphics[width=0.6\linewidth]{images/news_data.png}
		\caption{Sample snapshot of scraped News data}
		\label{fig:news_data}
	\end{figurehere}
	
	\subsection{Data Pre-Processing} \label{data_preprocessing}
		\subsubsection{Method A \cite{metha}} \label{method_a}
		In this method, we started with utilizing word2vec \cite{w2v} for embedding the articles' text. However, this did not work for us. That is, the model trained really poorly. We believe the reason to be that we are giving every word (or word $\rightarrow$ vec, which is deterministic) the same value across an article, which would freak out the training and converge everything to 0. This is not helpful.\\
		We changed our direction after experimenting with word2vec \cite{w2v}. We passed the scraped articles as input, while padding and truncating them at 580 characters. We pass this input set through bert-large-uncased tokenizer \cite{bert}. This allowed us to process the data and utilize it with existing functionalities of Bert \cite{bert} to develop a sentiment analyst model via bert-large-uncase \cite{bert}. More on this will be discussed in \cref{model_dev}.
		
		\subsubsection{Method B \cite{methb}} \label{method_b}
		In this method, local computing resources were insufficient. Hence, we worked and developed in Google Colab \cite{colab}.This allowed us to utilize cloud computing capabilities of Google Infrastructure, reducing limitations introduced by local computing resources. We experimented with a different technique from \enquote{Method A} \cite{metha}. Main differences include, but are not limited to, focusing on a singular stock to start (ServiceNow), leaning into scikit-learn's MLP model \cite{skmlp}, and utilizing Gensim's CBOW model \cite{gensim} to generate text embeddings rather Bert \cite{bert}. We also utilized NLTK \cite{nltk} to remove stopwords, cleaning up the web scraped news articles. We then split the cleaned articles' data via TimeSeriesSplit \cite{tssplit}. This data set was then tokenized via NLTK tokenizer \cite{nltk}, and then embedded via Gensim's CBOW \cite{gensim}.
			
	\subsection{Sentiment Analysis and Model Development} \label{model_dev}
	
		\subsubsection{Method A \cite{metha}} \label{method_a}
		In \cref{data_preprocessing} we had tokenized our news articles (input data) via bert-large-uncased tokenizer \cite{bert}. We processed this tokenized data through bert-large-uncased \cite{bert} with an added layer of 256 neurons, 0.3 dropout as the last head, and early stopping enabled. We froze all weights except for the 256 neurons. The model trained for 129 epochs with 3.33 training loss and 0.053 validation loss, as shown in \cref{fig:bert_training}.
		
		\mbox{}
		\begin{figurehere}
			\centering
			\includegraphics[width=0.95\linewidth]{images/bert_training_mac.png}
			\caption{Bert Model Training}
			\label{fig:bert_training}
		\end{figurehere}
				
		The output of the model is a prediction of \% increase in returns from previous day, with labels of current open and yesterday open. The calculation for rate of return being $\frac{\text{current open - yesterday open}}{\text{yesterday open}}$.
		
		\subsubsection{Method B \cite{methb}} \label{method_b}
		In \cref{data_preprocessing} we had tokenized our dataset via NLTK tokenizer \cite{nltk} and embedded it via Gensim's CBOW \cite{gensim}. We processed this data through scikit-learn's MLP model \cite{skmlp}, with two hidden layers, with the first hidden layer having 100 neurons and the second hidden layer having 50 neurons. We then set the model to train, and normalized open values to obtain a regression value. We will discuss metrics in \cref{model_eval}.
	
	\subsection{Model Evaluation} \label{model_eval}
		\subsubsection{Method A \cite{metha}} \label{method_a}
		The model trained for 129 epochs with 3.33 training loss and 0.053 validation loss, as shown in \cref{fig:bert_training}. Further evaluation and testing is to be completed in future work, as discussed in \cref{conclusion}.
		
		\subsubsection{Method B \cite{methb}} \label{method_b}
		We reviewed loss values post training of the model. We got mean squared error (MSE) with the value 0.0214 and mean absolute error (MAE) with the value 0.1277, which are quite good, albeit only for ServiceNow stock \cite{snow}. Further information on metrics is shown in \cref{fig:methbloss}.
		
		\mbox{}\\
		\begin{figurehere}
			\centering
			\includegraphics[width=0.7\linewidth]{images/methbloss.png}
			\caption{Method B Metrics}
			\label{fig:methbloss}
		\end{figurehere}
		\mbox{}\\
		
		We also plotted a trends graph of normalized data, over time, as discussed in \cref{model_dev}. This was done in order to be used a reference for future model enhancement and usage to determine whether our model is predicting buy vs sell in line with the trends. This graph is shown in \cref{fig:methbdatanorm}.\\		
		We also attempted to run predictions on the built model through some test cases, for which the results are shown in \cref{fig:methbpred}.\\
		Though the model is not complete, it is showing promising results, with next steps being to expand the model to all the stocks in our dataset instead of just ServiceNow \cite{snow}.		
		
		\mbox{}\\
		\begin{figurehere}
			\centering
			\includegraphics[width=\linewidth]{images/methbdatanorm.png}
			\caption{Method B Normalized Data Trends}
			\label{fig:methbdatanorm}
		\end{figurehere}
		
		\mbox{}\\
		\begin{figurehere}
			\centering
			\includegraphics[width=\linewidth]{images/methbpred.png}
			\caption{Method B Prediction Tests}
			\label{fig:methbpred}
		\end{figurehere}
		
	
	\section{Conclusion and Future Work} \label{conclusion}
	\iffalse
	Sentiment analysis is a valuable task in today's world filled with data and information. With development in AI and ML technologies, simplification and analysis of textual data in the realm of NLP holds tremendous value and importance. The stock market is a strong indicator of how the economy is performing and the daily happenings and news drive the stock market. Though we may not believe an event to be impactful to us directly, it could have profound impact depending on how the stock market accounts for said event. Individuals respond to stock market trends in relation to how safe or at risk they fell after a news event, which then drives the economy. We all strive for a stable economy in order to plan for our future and have a sense of security that our future will be safe. In order for this to be the case, we need to better understand the stock market. Hence, we need to better understand the sentiment of news.\\
	\fi
	For this assignment, it took us quite a bit of time to gather data. Additionally, it took us time to understand what technology and tools to utilize to process the data and develop a model that could provide us sentiment analysis. Though we were able to develop a model, it is not deployment ready. Our future work is to utilize the data, that we spent a lot of time to gather, and the existing development to better analyze news data in real-time. This work could then be further enhanced with the trends data and models that stock market analysts use in real-time to forecast market trends. Having such a model would provide tremendous value in better understanding and staying up-to-date with market trends.
	
	\section{Acknowledgements} \label{acknowledgements}
	We would like to acknowledge Dr. Jin \cite{rjin} for ML, classification and probability distribution concepts, Professor Avery \cite{avery} for MLP concepts, and Dr. Ryu \cite{cryu} for scraper, stock information downloading process and assignment information.

\bibliographystyle{IEEEtran}
\bibliography{references}

\end{flushleft}
%\end{multicols}
\end{document}