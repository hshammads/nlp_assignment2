# -*- coding: utf-8 -*-
"""488 - stocks project w/ word2vec

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1il2o8ghctNmrdsqSu6QVBzExEzmgHteV
"""

import pandas as pd
import numpy as np
import seaborn as sns

news_data = pd.read_csv('news_data_w_norm_with_embeddings.csv')

import re
import nltk
nltk.download('stopwords')

def cleanArticle(article):
    text = article

    # TEXT-CLEANING
    text = text.lower()
    text = text.strip()  # leading/trailing spaces
    text = re.sub(r'\s+', ' ', text)  # Replace multiple spaces with a single space

    # Remove non-alphabetic characters, URLs, or any other custom cleaning
    text = re.sub(r'http\S+', '', text)  # URLs
    text = re.sub(r'[^a-zA-Z\s]', '', text)  # non-alphabetic characters

    # Remove stopwords
    stop_words = set(nltk.corpus.stopwords.words('english'))
    words = text.split()
    words = [word for word in words if word not in stop_words]
    text = ' '.join(words)

    return text

news_data['Company'].value_counts()

news_meta_data = news_data[news_data['Company'] == 'NOW']
news_meta_data['Cleaned_Article'] = news_meta_data['Article'].apply(cleanArticle)
news_meta_data.drop(columns=['Article', 'Embeddings'],inplace=True)

news_meta_data.head()

"""---

# Word2Vec
"""

from gensim.models import Word2Vec
import gensim
from nltk.tokenize import sent_tokenize, word_tokenize
import warnings

warnings.filterwarnings(action='ignore')

def compute_embedding(text, model):
    words = text.split()  # Tokenize the text
    vectors = [model.wv[word] for word in words if word in model.wv]
    return np.mean(vectors, axis=0) if vectors else np.zeros(model.vector_size)

news_meta_data = news_meta_data.sort_values(by="Date")
sentences = news_meta_data['Cleaned_Article'].apply(lambda x: x.split()).tolist()

from sklearn.model_selection import TimeSeriesSplit
from sklearn.metrics import mean_squared_error

tscv = TimeSeriesSplit(n_splits=5)
for train_index, test_index in tscv.split(news_meta_data):
    train, test = news_meta_data.iloc[train_index], news_meta_data.iloc[test_index]

    train_sentences = train['Cleaned_Article'].apply(lambda x: x.split()).tolist()

    model = Word2Vec(train_sentences, min_count=1, vector_size=100, window=5)

    train['Embeddings'] = train['Cleaned_Article'].apply(lambda x: compute_embedding(x, model))
    test['Embeddings'] = test['Cleaned_Article'].apply(lambda x: compute_embedding(x, model))

    X_train = np.array(train['Embeddings'].tolist())
    y_train = train['Normalized'].values
    X_test = np.array(test['Embeddings'].tolist())
    y_test = test['Normalized'].values

# CBOW model
# model = gensim.models.Word2Vec(sentences, min_count=1,vector_size=100, window=5)

# Skip Gram model
# model = gensim.models.Word2Vec(data, min_count=1, vector_size=100, window=5, sg=1)

# news_data['Embeddings'] = news_data['Cleaned_Article'].apply(lambda x: compute_embedding(x, model))

"""---

# Multilayer Perceptron
"""

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.neural_network import MLPRegressor
from sklearn.metrics import mean_squared_error
from keras.models import Sequential
from keras.layers import Dense

mlp = MLPRegressor(hidden_layer_sizes=(100, 50), max_iter=1000, random_state=42)
mlp.fit(X_train, y_train)

from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import r2_score
from sklearn.metrics import accuracy_score
from sklearn.metrics import mean_absolute_percentage_error


mlp_predictions = mlp.predict(X_test)
mlp_mse = mean_squared_error(y_test, mlp_predictions)
mlp_mae = mean_absolute_error(y_test, mlp_predictions)
mlp_r2 = r2_score(y_test, mlp_predictions)
mape = mean_absolute_percentage_error(y_test, mlp_predictions)

print(f"Mean Squared Error (MSE): {mlp_mse}")
print(f"Mean Absolute Error (MAE): {mlp_mae}")
print(f"R-squared (R2): {mlp_r2}")
print(f'MAPE: {mape}')

# print(news_data['Normalized'])
print(news_meta_data)

news_data_dates_normalized = news_meta_data[['Date', 'Normalized']]
news_data_dates_normalized.drop_duplicates(inplace=True)

news_data_dates_normalized

import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(10, 6))
plt.plot(news_data_dates_normalized['Date'], news_data_dates_normalized['Normalized'], marker='o', linestyle='-', color='b')

plt.title('Normalized Values Over Time')
plt.xlabel('Date')
plt.ylabel('Normalized')
plt.xticks(rotation=45)
plt.grid(True)

# X_test is a list of embeddings
test_case = "ServiceNow, Inc. (NOW): Needham Maintains $1,150 Price Target, Highlights GenAI Growth Potential with Xanadu."
test_case = cleanArticle(test_case)
test_case_embedding = compute_embedding(test_case, model)
test_case_embedding = np.array(test_case_embedding).reshape(1, -1)

predicted_value = mlp.predict(test_case_embedding)
print(f"Predicted value: {predicted_value[0]}")

# X_test is a list of embeddings
test_case = "ServiceNow (NOW) Is Considered a Bad Investment by Brokers: Is That True?."
test_case = cleanArticle(test_case)
test_case_embedding = compute_embedding(test_case, model)
test_case_embedding = np.array(test_case_embedding).reshape(1, -1)

predicted_value = mlp.predict(test_case_embedding)
print(f"Predicted value: {predicted_value[0]}")

''' TODO: Use last stock-price date as a metric and determine how much we stock we or sell/buy. '''